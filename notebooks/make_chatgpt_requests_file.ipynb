{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogue_info(example: str) -> Tuple[int, str, bool]:\n",
    "    \"\"\"\n",
    "    Extract the length of a dialogue and the target speaker from an example.\n",
    "\n",
    "    :param example: Generated output from ChatGPT with the following format:\n",
    "    A1: I was at the port for 4 hours.\n",
    "    A2:\n",
    "    [with time elapsed]\n",
    "    A: I just arrived at the port now.\n",
    "    B: What are you going to do?\n",
    "    A: I'm going to take a walk around the port.\n",
    "    B: I see. Have fun.\n",
    "    [4 hours later]\n",
    "    A: The view was amazing! I took a lot of pictures.\n",
    "\n",
    "    [without time elapsed]\n",
    "    A: I'll send you some pictures after I get back.\n",
    "    :return: (dialogue length, the speaker of the target (last) message, whether the example is valid)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dialogue = example.split(\"[with time elapsed]\")[1]\n",
    "\n",
    "        context = dialogue.split(\"later]\\n\")[0].split(\"[\")[0].strip()\n",
    "        length = len(context.split(\"\\n\")) + 1\n",
    "        delayed_response = dialogue.split(\"later]\\n\")[1].split(\"\\n\")[0].strip()\n",
    "        immediate_response = dialogue.split(\"[without time elapsed]\\n\")[1].split(\"\\n\")[0].strip()\n",
    "\n",
    "        target_speaker = delayed_response[0]\n",
    "        is_valid = immediate_response[0] == target_speaker\n",
    "    except Exception as e:\n",
    "        return 0, \"\", False\n",
    "\n",
    "    return length, target_speaker, is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resources/atomic_2020_narratives.jsonl\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"../results/gpt-4-0125-preview/1shot/atomic_2020_dialogues.jsonl\") as f:\n",
    "    fewshot_examples = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = []\n",
    "for _ in range(len(data)):\n",
    "    is_valid = False\n",
    "    while not is_valid:\n",
    "        example = random.choice(fewshot_examples)\n",
    "        length, target_speaker, is_valid = extract_dialogue_info(example[\"generated\"])\n",
    "    other_speaker = \"B\" if target_speaker == \"A\" else \"A\"\n",
    "\n",
    "    # fmt: off\n",
    "    system_prompt = (\n",
    "        \"Given a narrative sentence containing an event and a question asking how long did the event last followed by the answer, follow the instructions below.\\n\\n\"\n",
    "        \"[Instruction]\\n\"\n",
    "        \"1. Combine the question and the answer into one declarative sentence to state the event duration.\\n\"\n",
    "        \"2. Based on the narrative, create an instant-messaging-style dialogue between two speakers following the conditions below.\\n\\n\"\n",
    "        \"[Condition]\\n\"\n",
    "        f\"1. A dialogue must be {length} utterances long.\\n\"\n",
    "        f\"2. {target_speaker} is messaging with {other_speaker} in the middle of the event, while {other_speaker} is not experiencing it.\\n\"\n",
    "        f\"3. After {other_speaker}'s last message, add \\\"[{{time_elapsed}} later]\\\", where time_elapsed is the time specified in the declarative sentence.\\n\"\n",
    "        f\"4. Suppose the same amount of time has passed in real world when {target_speaker} responds, so that {target_speaker} sends a message which is natural and appropriate considering the time passed.\\n\"\n",
    "        f\"5. Delete the \\\"[{{time_elapsed}} later]\\\" message and create another version of {target_speaker}'s last utterance as if no time has passed. Note that the former dialogue history remains intact.\\n\"\n",
    "        \"[/Condition]\\n\"\n",
    "        \"[/Instruction]\\n\\n\"\n",
    "        \"[Example]\\n\"\n",
    "        f\"{example['user_prompt']}\\n\"\n",
    "        f\"{example['generated']}\\n\"\n",
    "        \"[/Example]\"\n",
    "    )\n",
    "    # fmt: on\n",
    "    system_prompts.append(system_prompt)\n",
    "\n",
    "user_prompts = [f\"Narrative: {d['narrative']}\\nQuestion: {d['question']}\\nAnswer: {d['answer']}\\n\" for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo-0125\"\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "frequency_penalty = 0.0\n",
    "presence_penalty = 0.0\n",
    "max_tokens = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    for system_prompt, user_prompt in zip(system_prompts, user_prompts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resources/gpt-3.5-turbo-0125_requests_atomic_2020.jsonl\", \"w\") as f:\n",
    "    for message in messages:\n",
    "        f.write(json.dumps(message) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
